#!/usr/bin/env python3

import argparse
import datetime
import html.parser
import http.server
import json
import pathlib
import re
import socketserver
import sqlite3
import subprocess
import sys
import tempfile
import textwrap
import threading
import typing
import urllib.error
import urllib.parse
import urllib.request
import uuid
import webbrowser


class AgeEncryption:
    @staticmethod
    def generate_keypair() -> tuple[str, str]:
        try:
            result = subprocess.run(['age-keygen'], capture_output=True, text=True, check=True)
            lines = result.stdout.strip().split('\n')

            public_key = ''
            private_key = ''

            for line in lines:
                if line.startswith('# public key: '):
                    public_key = line.split(': ')[1]
                elif line.startswith('AGE-SECRET-KEY-'):
                    private_key = line.strip()
            if public_key and private_key:
                return public_key, private_key
            else:
                print('failed to parse age keypair output')
                return '', ''

        except (subprocess.CalledProcessError, FileNotFoundError, IndexError):
            print('failed to generate age keypair. age may not be installed.')
            return '', ''

    @staticmethod
    def encrypt_file(input_file: pathlib.Path, output_file: pathlib.Path, public_key: str) -> bool:
        try:
            subprocess.run(['age', '-r', public_key, '-o', str(output_file), str(input_file)], check=True)
            return True
        except subprocess.CalledProcessError:
            return False

    @staticmethod
    def decrypt_file(input_file: pathlib.Path, output_file: pathlib.Path, private_key: str) -> bool:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as temp_key_file:
            temp_key_file.write(private_key)
            temp_key_path = temp_key_file.name
        pathlib.Path(temp_key_path).chmod(0o600)

        try:
            subprocess.run(['age', '-d', '-i', temp_key_path, '-o', str(output_file), str(input_file)], check=True)
            return True
        except subprocess.CalledProcessError:
            return False
        finally:
            try:
                with open(temp_key_path, 'w') as f:
                    f.write('0' * 1024)
                pathlib.Path(temp_key_path).unlink()
            except OSError:
                pass


class GPGEncryption:
    @staticmethod
    def generate_keypair(name: str, email: str) -> str:
        try:
            gpg_commands = textwrap.dedent(f"""
                Key-Type: RSA
                Key-Length: 4096
                Subkey-Type: RSA
                Subkey-Length: 4096
                Name-Real: {name}
                Name-Email: {email}
                Expire-Date: 0
                %no-protection
                %commit
                """).strip()

            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(gpg_commands)
                temp_file = f.name

            result = subprocess.run(
                ['gpg', '--batch', '--gen-key', temp_file], capture_output=True, text=True, check=True
            )

            lines = result.stderr.split('\n')
            for line in lines:
                if 'key ' in line and 'created' in line:
                    key_id = line.split('key ')[1].split(' created')[0]
                    return key_id

            result = subprocess.run(['gpg', '--list-keys', '--with-colons'], capture_output=True, text=True, check=True)

            lines = result.stdout.split('\n')
            for line in reversed(lines):
                if line.startswith('pub:'):
                    parts = line.split(':')
                    if len(parts) > 4:
                        return parts[4]

            return ''

        except (subprocess.CalledProcessError, FileNotFoundError):
            print('failed to generate gpg keypair. gpg may not be installed.')
            return ''
        finally:
            if 'temp_file' in locals():
                pathlib.Path(temp_file).unlink(missing_ok=True)

    @staticmethod
    def encrypt_file(input_file: pathlib.Path, output_file: pathlib.Path, recipient: str = '') -> bool:
        try:
            if recipient:
                subprocess.run(
                    ['gpg', '--encrypt', '--recipient', recipient, '--output', str(output_file), str(input_file)],
                    check=True,
                )
            else:
                subprocess.run(
                    ['gpg', '--symmetric', '--cipher-algo', 'AES256', '--output', str(output_file), str(input_file)],
                    check=True,
                )
            return True
        except subprocess.CalledProcessError:
            return False

    @staticmethod
    def decrypt_file(input_file: pathlib.Path, output_file: pathlib.Path) -> bool:
        try:
            subprocess.run(['gpg', '--decrypt', '--output', str(output_file), str(input_file)], check=True)
            return True
        except subprocess.CalledProcessError:
            return False


class TitleExtractor(html.parser.HTMLParser):
    def __init__(self):
        super().__init__()
        self.in_title = False
        self.title = ''

    def handle_starttag(self, tag, attrs):
        if tag.lower() == 'title':
            self.in_title = True

    def handle_endtag(self, tag):
        if tag.lower() == 'title':
            self.in_title = False

    def handle_data(self, data):
        if self.in_title:
            self.title += data


class LinkManager:
    def __init__(self, notes_dir: pathlib.Path):
        self.notes_dir = notes_dir
        self.db_path = pathlib.Path.home() / '.dotfiles' / '.qn_links.db'
        self._init_db()

    def _init_db(self) -> None:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS links (
                url TEXT PRIMARY KEY,
                title TEXT,
                status TEXT,
                status_code INTEGER,
                last_checked TIMESTAMP,
                domain TEXT,
                first_seen TIMESTAMP
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS link_locations (
                url TEXT,
                file_path TEXT,
                line_number INTEGER,
                FOREIGN KEY (url) REFERENCES links(url) ON DELETE CASCADE
            )
        """)

        cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON link_locations(url)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_domain ON links(domain)')

        conn.commit()
        conn.close()

    def extract_urls_from_file(self, file_path: pathlib.Path) -> list[tuple[str, int]]:
        urls = []
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')

            in_code_block = False
            for line_num, line in enumerate(lines, 1):
                if line.strip().startswith('```'):
                    in_code_block = not in_code_block
                    continue

                if in_code_block:
                    continue

                url_pattern = r'https?://[^\s\)\]>]+'
                matches = re.finditer(url_pattern, line)
                for match in matches:
                    url = match.group(0)
                    url = url.rstrip('.,;:!?')
                    urls.append((url, line_num))

        except OSError:
            pass

        return urls

    def fetch_url_metadata(self, url: str, timeout: int = 5) -> tuple[typing.Optional[str], str, int]:
        try:
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (compatible; qn-links/1.0)'})

            with urllib.request.urlopen(req, timeout=timeout) as response:
                status_code = response.getcode()
                content_type = response.headers.get('Content-Type', '')

                title = None
                if 'text/html' in content_type:
                    html_content = response.read().decode('utf-8', errors='ignore')
                    parser = TitleExtractor()
                    parser.feed(html_content)
                    title = parser.title.strip() if parser.title else None

                return title, 'active', status_code

        except urllib.error.HTTPError as e:
            return None, 'dead', e.code
        except (urllib.error.URLError, TimeoutError, Exception):
            return None, 'dead', 0

    def catalog(self) -> None:
        print('scanning notes...')

        current_urls = {}
        markdown_files = list(self.notes_dir.rglob('*.md'))

        for file_path in markdown_files:
            relative_path = file_path.relative_to(self.notes_dir)
            urls = self.extract_urls_from_file(file_path)

            for url, line_num in urls:
                if url not in current_urls:
                    current_urls[url] = []
                current_urls[url].append((str(relative_path), line_num))

        print(f'found {len(current_urls)} unique links across {len(markdown_files)} notes\n')

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('SELECT url FROM links')
        db_urls = {row[0] for row in cursor.fetchall()}

        orphaned_urls = db_urls - set(current_urls.keys())

        if orphaned_urls:
            print(f'cleaning up:')
            print(f'  removed {len(orphaned_urls)} orphaned links no longer in notes\n')
            for url in orphaned_urls:
                cursor.execute('DELETE FROM links WHERE url = ?', (url,))
                cursor.execute('DELETE FROM link_locations WHERE url = ?', (url,))

        new_urls = set(current_urls.keys()) - db_urls

        print('updating index:')
        if new_urls:
            print(f'  fetching metadata for {len(new_urls)} new links...')

            for i, url in enumerate(new_urls, 1):
                title, status, status_code = self.fetch_url_metadata(url)
                domain = urllib.parse.urlparse(url).netloc
                now = datetime.datetime.now().isoformat()

                cursor.execute(
                    """
                    INSERT INTO links (url, title, status, status_code, last_checked, domain, first_seen)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                    (url, title, status, status_code, now, domain, now),
                )

                if i % 10 == 0:
                    print(f'    processed {i}/{len(new_urls)}...')

        cursor.execute('DELETE FROM link_locations')
        for url, locations in current_urls.items():
            for file_path, line_num in locations:
                cursor.execute(
                    """
                    INSERT INTO link_locations (url, file_path, line_number)
                    VALUES (?, ?, ?)
                """,
                    (url, file_path, line_num),
                )

        conn.commit()
        conn.close()

        print(f'  {len(new_urls)} new links added')
        print(f'  {len(current_urls) - len(new_urls)} existing links updated')
        print('\ncatalog complete!')

    def audit(self) -> None:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT url, COUNT(*) as count
            FROM link_locations
            GROUP BY url
            HAVING count > 1
            ORDER BY count DESC
        """)

        duplicates = cursor.fetchall()

        if duplicates:
            print(f'DUPLICATE LINKS ({len(duplicates)} found):\n')

            for url, count in duplicates:
                cursor.execute(
                    """
                    SELECT title, status, status_code
                    FROM links
                    WHERE url = ?
                """,
                    (url,),
                )

                link_data = cursor.fetchone()
                title, status, status_code = link_data if link_data else (None, 'unknown', 0)

                status_display = f'[{status.upper()}]' if status == 'active' else f'[{status_code} {status.upper()}]'
                print(f'{url} {status_display}')

                if title:
                    print(f'  Title: "{title}"')

                print(f'  Found in {count} notes:')

                cursor.execute(
                    """
                    SELECT file_path, line_number
                    FROM link_locations
                    WHERE url = ?
                    ORDER BY file_path
                """,
                    (url,),
                )

                locations = cursor.fetchall()
                for file_path, line_num in locations:
                    print(f'    - {file_path} (line {line_num})')

                print()
        else:
            print('no duplicate links found\n')

        print('checking link status...')
        cursor.execute('SELECT url, title, status_code FROM links WHERE status = "dead"')
        dead_links = cursor.fetchall()

        if dead_links:
            print(f'\nDEAD LINKS ({len(dead_links)} found):\n')

            for url, title, status_code in dead_links:
                print(f'{url} [{status_code} NOT FOUND]')
                if title:
                    print(f'  Title: "{title}"')

                cursor.execute(
                    """
                    SELECT file_path, line_number
                    FROM link_locations
                    WHERE url = ?
                """,
                    (url,),
                )

                locations = cursor.fetchall()
                if locations:
                    print('  Found in:')
                    for file_path, line_num in locations:
                        print(f'    - {file_path} (line {line_num})')

                print()
        else:
            print('no dead links found')

        conn.close()

    def list_links(self, dead_only: bool = False, domain: typing.Optional[str] = None) -> None:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = 'SELECT url, title, status, status_code, domain FROM links WHERE 1=1'
        params = []

        if dead_only:
            query += ' AND status = "dead"'

        if domain:
            query += ' AND domain = ?'
            params.append(domain)

        query += ' ORDER BY domain, url'

        cursor.execute(query, params)
        links = cursor.fetchall()

        if not links:
            print('no links found')
            conn.close()
            return

        print(f'Links ({len(links)} found):\n')

        current_domain = None
        for url, title, status, status_code, link_domain in links:
            if link_domain != current_domain:
                if current_domain is not None:
                    print()
                print(f'[{link_domain}]')
                current_domain = link_domain

            status_display = f'[{status}]' if status == 'active' else f'[{status_code}]'
            print(f'  {url} {status_display}')
            if title:
                print(f'    "{title}"')

        conn.close()

    def browse_urls(self, dead_only: bool = False, domain: typing.Optional[str] = None) -> None:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = '''
            SELECT l.url, l.title, l.status, l.status_code, l.domain, COUNT(ll.url) as note_count
            FROM links l
            LEFT JOIN link_locations ll ON l.url = ll.url
            WHERE 1=1
        '''
        params = []

        if dead_only:
            query += ' AND l.status = "dead"'

        if domain:
            query += ' AND l.domain = ?'
            params.append(domain)

        query += ' GROUP BY l.url ORDER BY l.domain, l.url'

        cursor.execute(query, params)
        links = cursor.fetchall()
        conn.close()

        urls_data = []
        for url, title, status, status_code, link_domain, note_count in links:
            urls_data.append({
                'url': url,
                'title': title,
                'status': status,
                'status_code': status_code,
                'domain': link_domain,
                'note_count': note_count
            })

        template_path = pathlib.Path(__file__).parent / 'qn_browser_template.html'
        html_content = template_path.read_text()
        html_content = html_content.replace('{{URLS_DATA}}', json.dumps(urls_data))

        class RequestHandler(http.server.SimpleHTTPRequestHandler):
            def do_GET(self):
                self.send_response(200)
                self.send_header('Content-type', 'text/html')
                self.end_headers()
                self.wfile.write(html_content.encode())

            def log_message(self, format, *args):
                pass

        port = 8765
        with socketserver.TCPServer(('', port), RequestHandler) as httpd:
            url = f'http://localhost:{port}'
            print(f'opening browser at {url}')
            print('press ctrl+c to stop server')

            threading.Timer(1.0, lambda: webbrowser.open(url)).start()

            try:
                httpd.serve_forever()
            except KeyboardInterrupt:
                print('\nserver stopped')


def get_config_path() -> pathlib.Path:
    return pathlib.Path.home() / '.dotfiles' / '.qn.json'


def get_default_config() -> dict[str, str]:
    return {
        'notes_dir': '~/notes',
        'backup_dir': '~/notes_backups',
        'editor': 'code',
        'encryption_tool': 'age',
        'append_note': 'inbox.md',
        'age_public_key': '',
        'age_private_key': '',
        'gpg_recipient': '',
        'gpg_private_key': '',
    }


def load_config() -> dict[str, str]:
    config_path = get_config_path()
    if not config_path.exists():
        return get_default_config()

    try:
        with open(config_path) as f:
            config = json.load(f)
        return {**get_default_config(), **config}
    except (json.JSONDecodeError, OSError):
        return get_default_config()


def save_config(config: dict[str, str]) -> None:
    if not validate_config_paths(config):
        print('invalid configuration paths')
        sys.exit(1)
    config_path = get_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)

    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    config_path.chmod(0o600)


def expand_path(path_str: str) -> pathlib.Path:
    if not path_str or '..' in path_str:
        raise ValueError(f'invalid path: {path_str}')
    return pathlib.Path(path_str).expanduser().resolve()


def validate_config_paths(config: dict[str, str]) -> bool:
    try:
        notes_dir = expand_path(config['notes_dir'])
        backup_dir = expand_path(config['backup_dir'])
        if str(notes_dir) in ['/', '/root', '/etc'] or str(backup_dir) in ['/', '/root', '/etc']:
            print('refusing to use system directories for notes/backup')
            return False
        return True
    except (ValueError, OSError):
        return False


def get_notes_dir() -> pathlib.Path:
    config = load_config()
    notes_dir = expand_path(config['notes_dir'])
    notes_dir.mkdir(parents=True, exist_ok=True)
    return notes_dir


def open_in_editor(file_path: pathlib.Path, also_open_directory: bool = False) -> None:
    config = load_config()
    editor = config['editor']
    if not editor or ';' in editor or '&' in editor or '|' in editor:
        print(f'invalid editor command: {editor}')
        sys.exit(1)

    if also_open_directory:
        notes_dir = get_notes_dir()
        try:
            subprocess.run([editor, str(notes_dir), str(file_path)], check=True)
            return
        except subprocess.CalledProcessError:
            pass
        except FileNotFoundError:
            print(f"editor '{editor}' not found")
            sys.exit(1)

    try:
        subprocess.run([editor, str(file_path)], check=True)
    except subprocess.CalledProcessError:
        print(f'failed to open {file_path} with {editor}')
        sys.exit(1)
    except FileNotFoundError:
        print(f"editor '{editor}' not found")
        sys.exit(1)


def create_note() -> None:
    notes_dir = get_notes_dir()
    hex_id = uuid.uuid4().hex[:8]
    note_path = notes_dir / f'{hex_id}.md'

    note_path.touch()
    open_in_editor(note_path, also_open_directory=True)


def create_daily_note() -> None:
    notes_dir = get_notes_dir()
    today = datetime.date.today()
    daily_path = notes_dir / 'daily' / f'{today.strftime("%Y%m%d")}.md'

    daily_path.parent.mkdir(parents=True, exist_ok=True)
    daily_path.touch()

    open_in_editor(daily_path)


def create_weekly_note() -> None:
    notes_dir = get_notes_dir()
    today = datetime.date.today()
    week_start = today - datetime.timedelta(days=today.weekday())
    week_end = week_start + datetime.timedelta(days=6)

    weekly_dir = notes_dir / 'weekly'
    weekly_dir.mkdir(parents=True, exist_ok=True)

    weekly_filename = f'{week_start.strftime("%Y%m%d")}-{week_end.strftime("%Y%m%d")}.md'
    weekly_path = weekly_dir / weekly_filename

    weekly_path.touch()
    open_in_editor(weekly_path)


def create_monthly_note() -> None:
    notes_dir = get_notes_dir()
    today = datetime.date.today()
    monthly_path = notes_dir / 'monthly' / f'{today.strftime("%Y%m")}.md'

    monthly_path.parent.mkdir(parents=True, exist_ok=True)
    monthly_path.touch()

    open_in_editor(monthly_path)


def open_notes_directory() -> None:
    notes_dir = get_notes_dir()
    open_in_editor(notes_dir)


def backup_notes(encrypt: bool = False) -> None:
    config = load_config()
    notes_dir = expand_path(config['notes_dir'])
    backup_dir = expand_path(config['backup_dir'])

    if not notes_dir.exists():
        print('notes directory does not exist')
        return

    backup_dir.mkdir(parents=True, exist_ok=True)

    if encrypt:
        encryption_tool = config['encryption_tool']
        backup_file = backup_dir / 'notes_backup.tar.gz.enc'

        with tempfile.NamedTemporaryFile(suffix='.tar.gz', delete=False) as temp_file:
            temp_path = pathlib.Path(temp_file.name)
        try:
            subprocess.run(['tar', '-czf', str(temp_path), '-C', str(notes_dir.parent), notes_dir.name], check=True)
            success = False
            if encryption_tool == 'age':
                public_key = config.get('age_public_key', '')
                if not public_key:
                    print('age public key not configured in config')
                    return
                success = AgeEncryption.encrypt_file(temp_path, backup_file, public_key)
            elif encryption_tool == 'gpg':
                recipient = config.get('gpg_recipient', '')
                success = GPGEncryption.encrypt_file(temp_path, backup_file, recipient)

            if success:
                print(f'encrypted backup created: {backup_file}')
            else:
                print('backup encryption failed')
        except subprocess.CalledProcessError:
            print('backup creation failed')
        finally:
            temp_path.unlink(missing_ok=True)
    else:
        backup_file = backup_dir / 'notes_backup.tar.gz'
        try:
            subprocess.run(['tar', '-czf', str(backup_file), '-C', str(notes_dir.parent), notes_dir.name], check=True)
            print(f'backup created: {backup_file}')
        except subprocess.CalledProcessError:
            print('backup failed')


def decrypt_backup(output_dir: str = None) -> None:
    config = load_config()
    backup_dir = expand_path(config['backup_dir'])
    encryption_tool = config['encryption_tool']
    encrypted_backup = backup_dir / 'notes_backup.tar.gz.enc'

    if not encrypted_backup.exists():
        print(f'no encrypted backup found at {encrypted_backup}')
        return

    if output_dir is None:
        output_dir = str(backup_dir)
    else:
        output_dir = str(expand_path(output_dir))
        pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)

    decrypted_file = pathlib.Path(output_dir) / 'notes_backup.tar.gz'
    try:
        success = False
        if encryption_tool == 'age':
            private_key = config.get('age_private_key', '')
            if not private_key:
                print('age private key not configured in config')
                return
            success = AgeEncryption.decrypt_file(encrypted_backup, decrypted_file, private_key)
        elif encryption_tool == 'gpg':
            success = GPGEncryption.decrypt_file(encrypted_backup, decrypted_file)

        if success:
            print(f'backup decrypted to: {decrypted_file}')
            response = input('extract the decrypted archive? (y/N): ')
            if response.lower() == 'y':
                extract_dir = pathlib.Path(output_dir) / 'extracted_notes'
                extract_dir.mkdir(parents=True, exist_ok=True)
                subprocess.run(['tar', '-xzf', str(decrypted_file), '-C', str(extract_dir)], check=True)
                print(f'archive extracted to: {extract_dir}')
        else:
            print('decryption failed')
    except subprocess.CalledProcessError as e:
        print(f'extraction failed: {e}')
    except FileNotFoundError:
        print(f'{encryption_tool} tool not found')


def prompt_config_value(prompt_text: str, default_value: str, validator=None) -> str:
    while True:
        user_input = input(f'{prompt_text} [{default_value}]: ').strip()
        value = user_input if user_input else default_value
        if validator and not validator(value):
            continue
        return value


def initialize_config() -> None:
    config_path = get_config_path()

    if config_path.exists():
        response = input(f'config file already exists at {config_path}. overwrite? (y/N): ')
        if response.lower() != 'y':
            return

    print('initializing quick notes configuration...')
    print('press Enter to use default values shown in brackets.\n')
    defaults = get_default_config()

    notes_dir = prompt_config_value('notes directory', defaults['notes_dir'])
    backup_dir = prompt_config_value('backup directory', defaults['backup_dir'])
    editor = prompt_config_value('editor command', defaults['editor'])

    def validate_encryption_tool(value: str) -> bool:
        if value.lower() not in ['age', 'gpg']:
            print('please enter "age" or "gpg"')
            return False
        return True

    encryption_tool = prompt_config_value(
        'encryption tool (age/gpg)', defaults['encryption_tool'], validate_encryption_tool
    ).lower()

    append_note = prompt_config_value('append note filename', defaults['append_note'])

    age_public_key = defaults['age_public_key']
    age_private_key = defaults['age_private_key']
    gpg_recipient = defaults['gpg_recipient']
    gpg_private_key = defaults['gpg_private_key']

    if encryption_tool == 'age':
        if not age_public_key:
            print('generating age keypair...')
            public_key, private_key = AgeEncryption.generate_keypair()
            if public_key:
                age_public_key = public_key
                age_private_key = private_key
                print(f'generated age public key: {public_key}')
                print('age private key will be stored in config for decryption')
            else:
                age_public_key = input('age public key (leave empty to skip encryption): ').strip()
                age_private_key = input('age private key (leave empty to skip encryption): ').strip()
        else:
            age_public_key = input('age public key (leave empty to skip encryption): ').strip()
            age_private_key = input('age private key (leave empty to skip encryption): ').strip()

    elif encryption_tool == 'gpg':
        if not gpg_recipient:
            print('generating gpg keypair...')
            gpg_name = input('gpg key name [Quick Notes]: ').strip() or 'Quick Notes'
            gpg_email = input('gpg key email [notes@localhost]: ').strip() or 'notes@localhost'
            recipient = GPGEncryption.generate_keypair(gpg_name, gpg_email)
            if recipient:
                gpg_recipient = recipient
                print(f'generated gpg key id: {recipient}')
                print('gpg private key is stored in gpg keyring')
            else:
                gpg_recipient = input('gpg recipient (leave empty for symmetric encryption): ').strip()
        else:
            gpg_recipient = input('gpg recipient (leave empty for symmetric encryption): ').strip()

    config = {
        'notes_dir': notes_dir,
        'backup_dir': backup_dir,
        'editor': editor,
        'encryption_tool': encryption_tool,
        'append_note': append_note,
        'age_public_key': age_public_key,
        'age_private_key': age_private_key,
        'gpg_recipient': gpg_recipient,
        'gpg_private_key': gpg_private_key,
    }

    save_config(config)
    print(f'\nconfiguration initialized at {config_path}')

    notes_dir_path = expand_path(notes_dir)
    notes_dir_path.mkdir(parents=True, exist_ok=True)
    print(f'notes directory created at {notes_dir_path}')


def append_to_note(text: str) -> None:
    config = load_config()
    notes_dir = get_notes_dir()
    append_file = notes_dir / config['append_note']
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    entry = f'\n### {timestamp}\n\n{text}\n\n'

    if not append_file.exists():
        append_file.write_text(f'# {config["append_note"]}\n')
    with open(append_file, 'a') as f:
        f.write(entry)
    print(f'appended to {append_file}')


def count_words_in_file(file_path: pathlib.Path) -> int:
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        words = re.findall(r'\b\w+\b', content)
        return len(words)
    except OSError:
        return 0


def extract_tags_from_file(file_path: pathlib.Path) -> set[str]:
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        tags = set(re.findall(r'#(\w+)', content))
        return tags
    except OSError:
        return set()


def get_file_creation_date(file_path: pathlib.Path) -> datetime.datetime:
    try:
        stat = file_path.stat()
        return datetime.datetime.fromtimestamp(stat.st_ctime)
    except OSError:
        return datetime.datetime.min


def show_statistics() -> None:
    notes_dir = get_notes_dir()

    if not notes_dir.exists():
        print('notes directory does not exist')
        return

    markdown_files = list(notes_dir.rglob('*.md'))

    if not markdown_files:
        print('no markdown files found')
        return

    total_words = 0
    all_tags: set[str] = set()
    total_size = 0
    creation_dates: list[datetime.datetime] = []

    for file_path in markdown_files:
        total_words += count_words_in_file(file_path)
        all_tags.update(extract_tags_from_file(file_path))
        try:
            total_size += file_path.stat().st_size
        except OSError:
            pass

        creation_dates.append(get_file_creation_date(file_path))

    creation_dates.sort()
    earliest = creation_dates[0] if creation_dates else datetime.datetime.min
    latest = creation_dates[-1] if creation_dates else datetime.datetime.min
    size_mb = total_size / (1024 * 1024)

    print('notes statistics:')
    print(f'  total files: {len(markdown_files)}')
    print(f'  total words: {total_words:,}')
    print(f'  unique tags: {len(all_tags)}')
    print(f'  total size: {size_mb:.2f} MB')

    if earliest != datetime.datetime.min:
        print(f'  Earliest note: {earliest.strftime("%Y-%m-%d")}')
        print(f'  Latest note: {latest.strftime("%Y-%m-%d")}')
    if all_tags:
        sorted_tags = sorted(all_tags)
        if len(sorted_tags) <= 10:
            print(f'  Tags: {", ".join(sorted_tags)}')
        else:
            print(f'  Top tags: {", ".join(sorted_tags[:10])}...')


def prune_empty_files() -> None:
    notes_dir = get_notes_dir()
    if not notes_dir.exists():
        print('notes directory does not exist')
        return

    empty_files = []
    for file_path in notes_dir.rglob('*.md'):
        try:
            if file_path.stat().st_size == 0:
                empty_files.append(file_path)
            else:
                content = file_path.read_text(encoding='utf-8', errors='ignore')
                if not content.strip():
                    empty_files.append(file_path)
        except OSError:
            continue

    if not empty_files:
        print('no empty files found')
        return

    removed_count = 0
    for file_path in empty_files:
        try:
            file_path.unlink()
            print(f'removed: {file_path}')
            removed_count += 1
        except OSError as e:
            print(f'failed to remove {file_path}: {e}')

    print(f'successfully removed {removed_count} empty file(s)')


def print_all_tags() -> None:
    notes_dir = get_notes_dir()
    markdown_files = list(notes_dir.rglob('*.md'))
    all_tags = set()
    for file_path in markdown_files:
        all_tags.update(extract_tags_from_file(file_path))
    if all_tags:
        print('unique tags:')
        print('\n'.join(sorted(all_tags, key=str.lower)))
    else:
        print('no tags found.')


def main() -> None:
    parser = argparse.ArgumentParser(description='Quick Notes CLI')
    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    subparsers.add_parser('daily', help='Create/open daily note')
    subparsers.add_parser('weekly', help='Create/open weekly folder')
    subparsers.add_parser('monthly', help='Create/open monthly note')
    subparsers.add_parser('open', help='Open notes directory')
    subparsers.add_parser('init', help='Initialize configuration')
    subparsers.add_parser('stats', help='Show note statistics')
    subparsers.add_parser('tags', help='Print all unique tags in notes')

    backup_parser = subparsers.add_parser('backup', help='Backup notes')
    backup_parser.add_argument('--encrypt', action='store_true', help='Encrypt backup')

    decrypt_parser = subparsers.add_parser('decrypt', help='Decrypt backup')
    decrypt_parser.add_argument('--output-dir', help='Output directory for decrypted files (default: backup directory)')

    subparsers.add_parser('prune', help='Remove empty note files')

    append_parser = subparsers.add_parser('append', aliases=['a'], help='Append text to note')
    append_parser.add_argument('text', nargs='+', help='Text to append')

    url_parser = subparsers.add_parser('url', help='Manage URLs in notes')
    url_subparsers = url_parser.add_subparsers(dest='url_command', help='URL commands')

    url_subparsers.add_parser('catalog', help='Scan notes and build/update link index')
    url_subparsers.add_parser('audit', help='Check for duplicate and dead links')

    ls_parser = url_subparsers.add_parser('ls', help='List all links')
    ls_parser.add_argument('--dead', action='store_true', help='Show only dead links')
    ls_parser.add_argument('--domain', type=str, help='Filter by domain')

    browse_parser = url_subparsers.add_parser('browse', help='Open web interface to browse URLs')
    browse_parser.add_argument('--dead', action='store_true', help='Show only dead links')
    browse_parser.add_argument('--domain', type=str, help='Filter by domain')

    args = parser.parse_args()

    if args.command == 'daily':
        create_daily_note()
    elif args.command == 'weekly':
        create_weekly_note()
    elif args.command == 'monthly':
        create_monthly_note()
    elif args.command == 'open':
        open_notes_directory()
    elif args.command == 'backup':
        backup_notes(args.encrypt)
    elif args.command == 'decrypt':
        decrypt_backup(args.output_dir)
    elif args.command == 'prune':
        prune_empty_files()
    elif args.command == 'init':
        initialize_config()
    elif args.command in ['append', 'a']:
        text = ' '.join(args.text)
        append_to_note(text)
    elif args.command == 'stats':
        show_statistics()
    elif args.command == 'tags':
        print_all_tags()
    elif args.command == 'url':
        notes_dir = get_notes_dir()
        link_manager = LinkManager(notes_dir)

        if args.url_command == 'catalog':
            link_manager.catalog()
        elif args.url_command == 'audit':
            link_manager.audit()
        elif args.url_command == 'ls':
            link_manager.list_links(dead_only=args.dead, domain=args.domain)
        elif args.url_command == 'browse':
            link_manager.browse_urls(dead_only=args.dead, domain=args.domain)
        else:
            print('usage: qn url {catalog,audit,ls,browse}')
            print('  catalog  - Scan notes and build/update link index')
            print('  audit    - Check for duplicate and dead links')
            print('  ls       - List all links (--dead, --domain)')
            print('  browse   - Open web interface to browse URLs')
    else:
        create_note()


if __name__ == '__main__':
    if sys.version_info < (3, 11):
        print('error: this script requires Python 3.11 or later')
        sys.exit(1)
    main()
